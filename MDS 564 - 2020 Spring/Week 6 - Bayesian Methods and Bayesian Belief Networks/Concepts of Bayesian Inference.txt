The following are the main concepts of the Bayesian Lab Test:

Bayesian networks can leverage intelligence from both humans and machines. This technology is also more interpretable then most machine learning techniques allowing analyst to learn information and build theories as a result of the model. Adding causal assumptions to a Bayesian network allows us to move from the constraints of association/correlation to understanding causality relationships of a function and the target variable Y.

Bayesian networks consist of DAG (directed acyclic graph) that represents a qualtative relationship (either informational or causal) and quantative local probability distributons. When a causal relationship is assumed, a conditional probability table is calculated for each feature given its parent. The output of a Bayesian network is the JPD (joint probability distribution) used to compute posterior probabilities of a subset of variables/condition given the evidence.

A Dynamic Bayesian Network (DBN) is used to keep track of features that change over time. It incorporates the historical state of features (t-1, t-2, etc.) and the current state (t) is used to determine the probability of the evidence variable given the state of the observed variables in its reasoning to produce a conditional probability distribution.

As aforementioned, a bayesian network can learn from human or machine intelligence. 
the conditional probability tables are estimated using the maximum likelihood calculation from observed data. The Bayesian Network itself is designed from expert knowledge (human intelligence). The data is used as evidence to update the distribution of hyperparameters. However, a machine can also learn to design the structure of the Bayesian network using two methods: constraint-based algorithms and score-based algorithms. In constraint-based algorithms, links are added or deleted in response to statistical tests that identifies marginal and conditional independencies. Score-based algorithms produces candiate networks using available data.

In regards to the Learning to Love Bayesian Statistics video featuring Allen Downey, he combats unapproving myths of the bayesian method. One myth was that it did the same thing as inference statistics. However, going through the above text from Bayesian Labs, we know that statistics fails to acknowledge causal relationships and answers questions in an abstract way. With a Bayesian Network, on the other hand, produces more interpretable results and is more flexible in assuming relationships between variables. I do think that continued research in Bayesian Network will produce more actionable insight then traditional statistics.