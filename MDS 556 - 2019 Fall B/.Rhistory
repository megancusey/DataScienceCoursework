sum(y*log(py) + (1-y)*log(1-py))
}
pnull <- mean(as.numeric(train$atRisk))
null.dev <- 2*loglikelihood(as.numeric(train$atRisk),pnull)
pnull
## [1] 0.01920912
null.dev
## [1] -2698.716 why did I get negative here?
model$null.deviance
## [1] 2698.716
pred <- predict(model, newdata=train, type="response")
resid.dev <- 2*loglikelihood(as.numeric(train$atRisk),pred)
resid.dev
model$deviance
pred <- predict(model, newdata=train, type="response")
##[1] 2462.992
testy <- as.numeric(test$atRisk)
testpred<-predict(model,newdata=test,type="response")
pnull.test <-mean(testy)
null.dev.test <- 2*loglikelihood(testy, pnull.test)
null.dev.test <- -2*loglikelihood(testy, pnull.test)
resid.dev.test <- -2*loglikelihood(testy,testpred)
pnull.test
## [1] 0.0172713
null.dev.test
## [1] 2110.91
resid.dev.test
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
###################################################################################
## THE FOLLOWING IS AN EXAMPLE OF A DECISION TREE W/O BAGGING
###################################################################################
rm(list = ls())
rm(list = ls())
setwd("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B/")
spamD <- read.table("spamD.tsv.txt", header=T, sep="\t")
## SPLIT INTO TEST/TRAIN
spamTrain <- subset(spamD, spamD$rgroup>=10)
spamTest <- subset(spamD, spamD$rgroup<10)
spamVars <- setdiff(colnames(spamD), list("rgroup","spam"))
spamVars
View(spamD)
View(spamD)
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(SalePrice >= 214000)
data$HighDollar <- +(data$SalePrice >= 214000)
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
data <- -data$SalePrice
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(data$SalePrice >= 214000)
data <- subset(data, select =-c("SalePrice"))
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
View(data)
View(data)
data <- subset(data, select =-c("SalePrice"))
data <- subset(data, select =-c("SalePrice"))
data <- subset(data, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(data$SalePrice >= 214000)
data <- subset(data, select =-c(SalePrice))
split <- .8
train <- splitData("train", split ,data)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(data$SalePrice >= 214000)
data <- subset(data, select =-c(SalePrice))
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(data$SalePrice >= 214000)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = SPAM DOCUMENTS
houseFormula <- as.formula(paste('HighDollar==1',
paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
## data preprocessing
source("data preprocessing.R", local = TRUE)
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- +(data$SalePrice >= 214000)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = SPAM DOCUMENTS
houseFormula <- as.formula(paste('HighDollar==1',
paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
test <- subset(data, select =-c(SalePrice))
test <- subset(data, select =-c(SalePrice))
train <- subset(data, select =-c(SalePrice))
test <- subset(test, select =-c(SalePrice))
train <- subset(train, select =-c(SalePrice))
accuracyMeasures(predict(treemodel, newdata=train),
train$HighDollar==1,
name="tree, training")
accuracyMeasures(predict(treemodel, newdata=test),
spamTest$HighDollar==1,
name="tree, test")
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar==1,
name="tree, test")
accuracyMeasures(predict(treemodel, newdata=train),
train$HighDollar==1,
name="tree, training")
## Use bootstrap samples that are the same size as the training set, with 100 trees.
ntrain <- dim(train)[1] ## returns # of rows in spamTrain (4143)
n <- ntrain
ntree <- 100
## BUILD THE BOOTSTRAP SAMPLES BY SAMPLING THE ROW INDICES OF TRAIN WITH REPLACEMENT
## EACH COLUMN OF THE MATRIX SAMPLES REPRESENT THE ROW INDICES INTO TRAIN THAT COMPRISE
## THE BOOTSTRAP SAMPLE.
samples <- sapply(1:ntree, ## loop 1-100
FUN = function(iter){
## obtain 100 samples, with replacement
sample(1:ntrain,
size=n,
replace=T)
})
## TRAIN THE INDIVIDUAL TREES FOR THE SAMPLES CREATED ABOVE AND RETURN THEM IN A LIST
treelist <- lapply(1:ntree,
FUN=function(iter){
samp <- samples[,iter];
rpart(houseFormula, train[samp,])
})
## Use predict.bag that assumes the underlying classifier to return decision probabilities
## instead of decisions
predict.bag <- function(treelist, newdata){
preds <- sapply(1:length(treelist),
FUN=function(iter) {
predict(treelist[[iter]], newdata=newdata)
})
predsums <- rowSums(preds)
predsums/length(treelist)
}
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar==1,
name="baggin, training")
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar==1,
name="bagging, test")
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar==1,
name="bagging, test")
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar==1,
name="baggin, training")
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar==1,
name="bagging, test")
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=data$HighDollar, ## dependent variable
netree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
library(randomForest)
set.seed(5123512)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=data$HighDollar, ## dependent variable
netree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=data$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=spamTrain[,spamVars], ## independent variables
y=spamTrain$spam, ## dependent variable
netree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=data$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar==1, name="random forest, train")
varImp <- importance(fmodel)
## importance returns a matrix of importance measures.
## the larger the value, the more important the feature is
varImp[1:10,]
fmodel
varImpPlot(fmodel, type=1)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar==1, name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='classification')[,'HighDollar'],
train$HighDollar==1, name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='response')[,'HighDollar'],
train$HighDollar==1, name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='vote')[,'HighDollar'],
train$HighDollar==1, name="random forest, train")
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- elseif(data$SalePrice >= 214000,"yes","no")
data$HighDollar <- ifelse(data$SalePrice >= 214000,"yes","no")
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(SalePrice))
test <- subset(test, select =-c(SalePrice))
train <- subset(train, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = SPAM DOCUMENTS
houseFormula <- as.formula(paste('HighDollar=="yes"',
paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
accuracyMeasures(predict(treemodel, newdata=train),
train$HighDollar==1,
name="tree, training")
accuracyMeasures(predict(treemodel, newdata=train),
train$HighDollar=="yes",
name="tree, training")
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar=="yes",
name="tree, test")
## Use bootstrap samples that are the same size as the training set, with 100 trees.
ntrain <- dim(train)[1] ## returns # of rows in train (1258)
n <- ntrain
ntree <- 100
## BUILD THE BOOTSTRAP SAMPLES BY SAMPLING THE ROW INDICES OF TRAIN WITH REPLACEMENT
## EACH COLUMN OF THE MATRIX SAMPLES REPRESENT THE ROW INDICES INTO TRAIN THAT COMPRISE
## THE BOOTSTRAP SAMPLE.
samples <- sapply(1:ntree, ## loop 1-100
FUN = function(iter){
## obtain 100 samples, with replacement
sample(1:ntrain,
size=n,
replace=T)
})
treelist
## TRAIN THE INDIVIDUAL TREES FOR THE SAMPLES CREATED ABOVE AND RETURN THEM IN A LIST
treelist <- lapply(1:ntree,
FUN=function(iter){
samp <- samples[,iter];
rpart(houseFormula, train[samp,])
})
## Use predict.bag that assumes the underlying classifier to return decision probabilities
## instead of decisions
predict.bag <- function(treelist, newdata){
preds <- sapply(1:length(treelist),
FUN=function(iter) {
predict(treelist[[iter]], newdata=newdata)
})
predsums <- rowSums(preds)
predsums/length(treelist)
}
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar=="yes",
name="baggin, training")
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar=="yes",
name="bagging, test")
library(randomForest)
set.seed(5123512)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
View(data)
View(data)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
sapply(data,function(x) sum(is.na(x)))
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
houseVars
data$HighDollar <- factor(data$HighDollar,
levels = c(  'Yes','No'))
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- ifelse(data$SalePrice >= 214000,"yes","no")
data$HighDollar <- factor(data$HighDollar)
data$HighDollar <- factor(data$HighDollar)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(SalePrice))
test <- subset(test, select =-c(SalePrice))
train <- subset(train, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = SPAM DOCUMENTS
houseFormula <- as.formula(paste('HighDollar=="yes"',
paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
library(randomForest)
set.seed(5123512)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
summary(data$SalePrice)
data$HighDollar <- ifelse(data$SalePrice >= 214000,"yes","no")
data$HighDollar <- factor(data$HighDollar)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(SalePrice))
test <- subset(test, select =-c(SalePrice))
train <- subset(train, select =-c(SalePrice))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = SPAM DOCUMENTS
houseFormula <- as.formula(paste('HighDollar=="yes"',
paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
accuracyMeasures(predict(treemodel, newdata=train),
train$HighDollar=="yes",
name="tree, training")
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar=="yes",
name="tree, test")
## Use bootstrap samples that are the same size as the training set, with 100 trees.
ntrain <- dim(train)[1] ## returns # of rows in train (1258)
n <- ntrain
ntree <- 100
## BUILD THE BOOTSTRAP SAMPLES BY SAMPLING THE ROW INDICES OF TRAIN WITH REPLACEMENT
## EACH COLUMN OF THE MATRIX SAMPLES REPRESENT THE ROW INDICES INTO TRAIN THAT COMPRISE
## THE BOOTSTRAP SAMPLE.
samples <- sapply(1:ntree, ## loop 1-100
FUN = function(iter){
## obtain 100 samples, with replacement
sample(1:ntrain,
size=n,
replace=T)
})
## TRAIN THE INDIVIDUAL TREES FOR THE SAMPLES CREATED ABOVE AND RETURN THEM IN A LIST
treelist <- lapply(1:ntree,
FUN=function(iter){
samp <- samples[,iter];
rpart(houseFormula, train[samp,])
})
## Use predict.bag that assumes the underlying classifier to return decision probabilities
## instead of decisions
predict.bag <- function(treelist, newdata){
preds <- sapply(1:length(treelist),
FUN=function(iter) {
predict(treelist[[iter]], newdata=newdata)
})
predsums <- rowSums(preds)
predsums/length(treelist)
}
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar=="yes",
name="baggin, training")
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar=="yes",
name="bagging, test")
library(randomForest)
set.seed(5123512)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=7, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=test[,houseVars], type='prob')[,'HighDollar'],
test$HighDollar=="yes", name="random forest, test")
predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar']
setwd("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B/")
spamD <- read.table("spamD.tsv.txt", header=T, sep="\t")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
houseVars
if (train$High == "yes")
print("Yes")
if (train$High == "yes")
print("Yes")
houseVars
accuracyMeasures(predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar'],
train$HighDollar=="yes", name="random forest, train")
predict(fmodel,
newdata=train[,houseVars], type='prob')[,'HighDollar']
accuracyMeasures(predict(fmodel,
newdata=test[,houseVars], type='prob')[,'HighDollar'],
test$HighDollar=="yes", name="random forest, test")
varImp <- importance(fmodel)
## importance returns a matrix of importance measures.
## the larger the value, the more important the feature is
varImp[1:10,]
varImpPlot(fmodel, type=1)
