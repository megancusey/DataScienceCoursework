##myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
autoplot(myts)
library(ggplot2)
##retaildata <- readxl::read_excel("retail.xlsx", skip=1)
##myts <- ts(retaildata[,"A3349873A"],
##           frequency=12, start=c(1982,4))
autoplot(myts)
View(retaildata)
View(retaildata)
library(ggplot2)
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
View(retaildata)
View(retaildata)
ggseasonplot(myts, year.labels=TRUE, year.labels.left=TRUE)
install.packages("installr")
library(installr) # install+load installr
updateR() # updating R.
rm(list = ls())
library(fma)
plot(fancy)
library(fpp2)
autoplot(fancy)
fancy_data < - fancy
fancy_data <- fancy
fancy_data
tslm(fancy ~ trend + season)
log.fancy <- log(fancy)
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
dummy.fest[3]
dummy.fest
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
## create dummy fest as a time series object
dummy.fest <- ts(dummy.fest, freq=12, start=c(1987,1))
## combine data
data <- data.frame(log.fancy, dummy.fest)
View(data)
View(data)
tslm(fancy ~ trend + season + dummy.fest data=data)
## combine data
model_data <- data.frame(log.fancy, dummy.fest)
tslm(fancy ~ trend + season + dummy.fest data=model_data)
tslm(fancy ~ trend + season + dummy.fest, data=model_data)
fit.fancy <- tslm(fancy ~ trend + season + dummy.fest, data=model_data)
autoplot(fancy, series="Data") +
autolayer(fitted(fit.fancy), series="Fitted") +
xlab("Year") + ylab("Sales")
plot(residuals(fit.fancy), type='p')
checkresiduals(fit.fancy)
dummy.fest[3] <- 0
## transform sales data with log of sales
log.fancy <- log(fancy)
## create dummy variable for surfing festival
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
## March 1987 didn't have a festival
dummy.fest[3] <- 0
## create dummy fest as a time series object
dummy.fest <- ts(dummy.fest, freq=12, start=c(1987,1))
## combine data
model_data <- data.frame(log.fancy, dummy.fest)
## create regression line
fit.fancy <- tslm(fancy ~ trend + season + dummy.fest, data=model_data)
## Plot regression model fitted vs data
autoplot(fancy, series="Data") +
autolayer(fitted(fit.fancy), series="Fitted") +
xlab("Year") + ylab("Sales") +
ggtitle("Fancy Sales/Year")
checkresiduals(fit.fancy)
plot(log.fancy)
autoplot(fit.fancy$residuals)
plot(as.numeric(fitted(fit.fancy)), residuals(fit), type='p')
plot(as.numeric(fitted(fit.fancy)), residuals(fit.fancy), type='p')
autoplot(fit.fancy$residuals)
boxplot(resid(fit.fancy) ~ cycle(resid(fit.fancy)))
summary(fit.fancy)
checkresiduals(fit.fancy)
future.data <- data.frame(dummy.fest = rep(0,36))
forecast.fancy <- forecast(fit.fancy, newdata=future.data)
View(future.data)
View(future.data)
View(forecast.fancy)
View(forecast.fancy)
plot(forecast.fancy)
summary(forecast.fancy)
data <- as.data.fram(forecast.fancy)
data <- exp(data) ## transform back from log
data <- as.data.frame(forecast.fancy)
data <- exp(data) ## transform back from log
data
data <- as.data.frame(forecast.fancy)
data <- exp(data) ## transform back from log
data
data <- as.data.frame(exp(forecast.fancy))
exp(forecast.fancy)
summary(forecast.fancy)
data <- as.data.frame(forecast.fancy)
data <- exp(data$forecast)
data <- exp(data$`Point Forecast`)
data
library(fma)
library(fpp2)
rm(list = ls())
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
autoplot(plastics)
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
decomp_plastics <- decompose(plastics, type="multiplicative")
autoplot(plastics,series="Data") +
autolayer(seasadj(decomp_plastics), series="Seasonally Adjusted") +
xlab("Year") + ylab("Monthly Sales")
plastics
outlier.plastics[1] <- 5000
outlier.plastics <- plastics
outlier.plastics[1] <- 5000
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.plastics <- plastics
outlier.plastics[1] <- 2500
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.plastics <- plastics
outlier.plastics[1] <- 1400
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
autoplot(plastics,series="Data") +
autolayer(seasadj(decomp_plastics), series="Seasonally Adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.middle.plastics <- plastics
outlier.middle.plastics
outlier.middle.plastics <- plastics
outlier.middle.plastics[30] <- 500
decompose.outlier.middle.plastics <- decompose(outlier.middle.plastics, type="multiplicative")
autoplot(outlier.middle.plastics, series="Data") +
autolayer(trendcycle(decompose.outlier.middle.plastics), series="trend") +
autolayer(seasadj(decompose.outlier.middle.plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.end.plastics <- plastics
outlier.end.plastics[59] <- 2000
decompose.outlier.end.plastics <- decompose(outlier.end.plastics, type="multiplicative")
autoplot(outlier.end.plastics, series="Data") +
autolayer(trendcycle(decompose.outlier.end.plastics), series="trend") +
autolayer(seasadj(decompose.outlier.end.plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
rm(list = ls())
library(fma)
library(fpp2)
books.original <- books
autoplot(books.original)
autoplot(books.original) +
xlab("Day") +
ylab("Book Sales") +
ggtitle("Daily Book Sales for Paperback and Hardcover Books")
books.ses = ses(books.original, h=5)
autoplot(books.ses) +
autolayer(fitted(session.ses), series="Fitted")
books.ses = ses(books.original, h=5)
books.ses = ses(books.original, h=5)
books.original
books.ses = ses(books.original(,'Paperback'), h=5)
books.ses = ses(books.original$Paperback, h=5)
books.ses = ses(books.original[,'Paperback'], h=5)
books.ses.paperback = ses(books.original[,'Paperback'], h=5)
books.ses.hardcover = ses(books.original[,'Hardcover'], h=5)
autoplot(books.ses) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted") +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
autoplot(books.original) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted") +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
autoplot(books.ses.paperback) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted")
autoplot(books.ses.hardcover) +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
round(accuracy(books.ses.paperback),2)
round(accuracy(books.ses.hardcover),2)
books.holt.paperback <- holt(books.original[,'Paperback'])
books.holt.hardcover <- holt(books.original[,'Hardcover'])
books.holt.paperback <- holt(books.original[,'Paperback'], h=4)
books.holt.hardcover <- holt(books.original[,'Hardcover'], h=4)
autoplot(books.original[,'Paperback']) +
autolayer(books.holt.paperback, series="Holt's Method", PI=FALSE)
autoplot(books.original[,'Hardcover']) +
autolayer(books.holt.hardcover, series="Holt's Method", PI=FALSE)
round(accuracy(books.holt.paperback),2)
round(accuracy(books.holt.hardcover),2)
round(accuracy(books.holt.hardcover),2)
View(books.ses.hardcover)
View(books.ses.hardcover)
autoplot(books.original[,'Paperback']) +
autolayer(books.holt.paperback, series="Holt's Method", PI=FALSE)
autoplot(books.ses.paperback) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted")
books.ses.paperback$upper[1,'95%']
books.ses.paperback$lower[1,'95%']
books.ses.paperback.rsme <- round(accuracy(books.ses.paperback),2)
## Paperback RSME = 33.64
books.ses.hardcover.rsme <- round(accuracy(books.ses.hardcover),2)
## Hardcover RSME = 31.93
books.holt.paperback.rsme <- round(accuracy(books.holt.paperback),2)
## Paperback RSME = 31.14
books.holt.hardcover.rsme <- round(accuracy(books.holt.hardcover),2)
## Hardcover RSME = 27.19
books.ses.paperback$mean+1.96*
##2
## The plastics data set consists of the monthly sales (in thousands)
## of product A for a plastics manufacturer for five years.
## a. Plot the time series of sales of product A.
##    Can you identify seasonal fluctuations and/or a trend-cycle?
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
books.ses.paperback$upper[1,'95%']
books.ses.paperback$lower[1,'95%']
books.ses.paperback$mean+1.96* books.ses.paperback.rsme
books.ses.paperback$mean-1.96* books.ses.paperback.rsme
books.ses.paperback$mean + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
s <- sqrt(books.holt.paperback$model$mse)
s
View(books.holt.hardcover)
low <- books.holt.paperback$mean[1] - 1.96 * s
books.holt.hardcover
books.holt.paperback
c(low = low, high= high)
high <- books.holt.paperback$mean[1] + 1.96 * s
c(low = low, high= high)
s <- sqrt(books.holt.hardcover$model$mse)
high <- books.holt.hardcover$mean[1] + 1.96 * s
low <- books.holt.hardcover$mean[1] - 1.96 * s
books.holt.hardcover
c(low = low, high= high)
books.holt.paperback.level <- holt(books.original[,'Paperback'], h=4, level =95)
books.holt.paperback.level
books.holt.paperback
books.holt.hardcover
books.holt.paperback
rm(list = ls())
library(expsmooth)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
austa
library(fpp2)
austa
autoplot(austa)
autoplot(usgdp)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
austa %>% Arima(order=c(0,1,0), include.constant = FALSE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa.auto.arima <- austa %>% auto.arima()
austa.auto.arima %>% summary()
## ARIMA(0,1,1) with drift
## AIC = -15.24
## AICc = -14.46
austa.auto.arima %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(2,1,0), include.constant = TRUE)
austa %>% Arima(order=c(2,1,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(2,1,3), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(0,0,1), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(0,0,1), include.constant = TRUE)
austa %>% Arima(order=c(0,0,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
## AICc 108.03
## AIC 107.28
austa %>% Arima(order=c(0,0,0), include.constant = TRUE)
austa %>% Arima(order=c(0,2,1), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
rm(list = ls())
autoplot(usgdp)
autoplot(BoxCox(usgpd, BoxCox.lambda(usgdp)))
## De-trend series and reduce variances
autoplot(BoxCox(usgdp, BoxCox.lambda(usgdp)))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggPacf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% sumamry()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,0), include.constant=FALSE)
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,2))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% checkresiduals()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast() %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast(h=10) %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>%  auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
usgdp %>% auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
getwd()
setwd("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B")
dataset<- read.csv("Wine.csv")
dataset
View(dataset)
View(dataset)
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.80)
training_set <- subset(dataset, split == TRUE)
test_Set <- subset(dataset, split == FALSE)
## Feature Scaling
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_Set[-14])
## Feature Scaling
training_set[-14] = scale(training_set[-14])
test_Set[-14] = scale(test_Set[-14])
View(test_Set)
View(test_Set)
View(training_set)
View(training_set)
installed.packages('caret')
library('caret')
installed.packages('caret')
install.packages("caret")
library(caret)
library(e1071)
install.packages("e1071")
library(caret)
library(e1071)
pca = preProcess(x=training_set[-14], method="pca",pcaComp = 2)
training_set = predict(pca,training_set)
training_set
training_set = training_set[c(2,3,1)]
training_set
test_Set = predict(pca,test_Set)
test_Set = test_Set[c(2,3,1)]
test_Set
## build a model
classifier = svm(formula = Customer_Sement ~ .,
data = training_set,
type="C-classification",
kernel="linear")
## build a model
classifier = svm(formula = Customer_Segment ~ .,
data = training_set,
type="C-classification",
kernel="linear")
y_pred = predict(classifier, newdata = test_Set[-3])
y_pred
## confusion matrix
cm = table(test_set[,3],y_pred)
## confusion matrix
cm = table(test_Set[,3],y_pred)
cm
install.packages("ElemStatLearn")
library(ElemStatLearn)
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Test set results
library(ElemStatLearn)
set = test_Set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
# Visualising the Test set results
library(ElemStatLearn)
set = test_Set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
library(ElemStatLearn)
set = test_Set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
