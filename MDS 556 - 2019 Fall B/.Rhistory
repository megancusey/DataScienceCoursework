print(paste('Accuracy for test Naive Bayes', accuracy_Test_nb))
#Naive Bayes
fitnb <- naiveBayes(survived~., data= data_train)
predictnb <- predict(fitnb, data_test)
table_mat_nb <- table(data_test$survived, predictnb)
table_mat_nb
accuracy_Test_nb <- sum(diag(table_mat_nb)) /sum(table_mat_nb)
print(paste('Accuracy for test Naive Bayes', accuracy_Test_nb))
print(paste('Accuracy for test', accuracy_Test))
print(paste('Accuracy for test Naive Bayes', accuracy_Test_nb))
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
rm(list = ls())
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
autoplot(myts)
library("ggplot2", lib.loc="~/R/win-library/3.3")
autoplot(myts)
autoplot()
autoplotretaildata([,"A3349873A"])
autoplot(retaildata[,"A3349873A"])
autoplot(myts)
ggseasonplot(myts, year.labels=TRUE, year.labels.left=TRUE)
myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
autoplot(myts)
library(ggplot2)
##retaildata <- readxl::read_excel("retail.xlsx", skip=1)
##myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
autoplot(myts)
library(ggplot2)
##retaildata <- readxl::read_excel("retail.xlsx", skip=1)
##myts <- ts(retaildata[,"A3349873A"],
##           frequency=12, start=c(1982,4))
autoplot(myts)
View(retaildata)
View(retaildata)
library(ggplot2)
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],
frequency=12, start=c(1982,4))
View(retaildata)
View(retaildata)
ggseasonplot(myts, year.labels=TRUE, year.labels.left=TRUE)
install.packages("installr")
library(installr) # install+load installr
updateR() # updating R.
rm(list = ls())
library(fma)
plot(fancy)
library(fpp2)
autoplot(fancy)
fancy_data < - fancy
fancy_data <- fancy
fancy_data
tslm(fancy ~ trend + season)
log.fancy <- log(fancy)
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
dummy.fest[3]
dummy.fest
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
## create dummy fest as a time series object
dummy.fest <- ts(dummy.fest, freq=12, start=c(1987,1))
## combine data
data <- data.frame(log.fancy, dummy.fest)
View(data)
View(data)
tslm(fancy ~ trend + season + dummy.fest data=data)
## combine data
model_data <- data.frame(log.fancy, dummy.fest)
tslm(fancy ~ trend + season + dummy.fest data=model_data)
tslm(fancy ~ trend + season + dummy.fest, data=model_data)
fit.fancy <- tslm(fancy ~ trend + season + dummy.fest, data=model_data)
autoplot(fancy, series="Data") +
autolayer(fitted(fit.fancy), series="Fitted") +
xlab("Year") + ylab("Sales")
plot(residuals(fit.fancy), type='p')
checkresiduals(fit.fancy)
dummy.fest[3] <- 0
## transform sales data with log of sales
log.fancy <- log(fancy)
## create dummy variable for surfing festival
dummy.fest = rep(0, length(fancy))
dummy.fest[seq_along(dummy.fest)%% 12 == 3] <- 1
## March 1987 didn't have a festival
dummy.fest[3] <- 0
## create dummy fest as a time series object
dummy.fest <- ts(dummy.fest, freq=12, start=c(1987,1))
## combine data
model_data <- data.frame(log.fancy, dummy.fest)
## create regression line
fit.fancy <- tslm(fancy ~ trend + season + dummy.fest, data=model_data)
## Plot regression model fitted vs data
autoplot(fancy, series="Data") +
autolayer(fitted(fit.fancy), series="Fitted") +
xlab("Year") + ylab("Sales") +
ggtitle("Fancy Sales/Year")
checkresiduals(fit.fancy)
plot(log.fancy)
autoplot(fit.fancy$residuals)
plot(as.numeric(fitted(fit.fancy)), residuals(fit), type='p')
plot(as.numeric(fitted(fit.fancy)), residuals(fit.fancy), type='p')
autoplot(fit.fancy$residuals)
boxplot(resid(fit.fancy) ~ cycle(resid(fit.fancy)))
summary(fit.fancy)
checkresiduals(fit.fancy)
future.data <- data.frame(dummy.fest = rep(0,36))
forecast.fancy <- forecast(fit.fancy, newdata=future.data)
View(future.data)
View(future.data)
View(forecast.fancy)
View(forecast.fancy)
plot(forecast.fancy)
summary(forecast.fancy)
data <- as.data.fram(forecast.fancy)
data <- exp(data) ## transform back from log
data <- as.data.frame(forecast.fancy)
data <- exp(data) ## transform back from log
data
data <- as.data.frame(forecast.fancy)
data <- exp(data) ## transform back from log
data
data <- as.data.frame(exp(forecast.fancy))
exp(forecast.fancy)
summary(forecast.fancy)
data <- as.data.frame(forecast.fancy)
data <- exp(data$forecast)
data <- exp(data$`Point Forecast`)
data
library(fma)
library(fpp2)
rm(list = ls())
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
autoplot(plastics)
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
decomp_plastics <- decompose(plastics, type="multiplicative")
autoplot(plastics,series="Data") +
autolayer(seasadj(decomp_plastics), series="Seasonally Adjusted") +
xlab("Year") + ylab("Monthly Sales")
plastics
outlier.plastics[1] <- 5000
outlier.plastics <- plastics
outlier.plastics[1] <- 5000
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.plastics <- plastics
outlier.plastics[1] <- 2500
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.plastics <- plastics
outlier.plastics[1] <- 1400
decompose_outlier_plastics <- decompose(outlier.plastics, type="multiplicative")
autoplot(outlier.plastics, series="Data") +
autolayer(trendcycle(decompose_outlier_plastics), series="trend") +
autolayer(seasadj(decompose_outlier_plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
autoplot(plastics,series="Data") +
autolayer(seasadj(decomp_plastics), series="Seasonally Adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.middle.plastics <- plastics
outlier.middle.plastics
outlier.middle.plastics <- plastics
outlier.middle.plastics[30] <- 500
decompose.outlier.middle.plastics <- decompose(outlier.middle.plastics, type="multiplicative")
autoplot(outlier.middle.plastics, series="Data") +
autolayer(trendcycle(decompose.outlier.middle.plastics), series="trend") +
autolayer(seasadj(decompose.outlier.middle.plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
outlier.end.plastics <- plastics
outlier.end.plastics[59] <- 2000
decompose.outlier.end.plastics <- decompose(outlier.end.plastics, type="multiplicative")
autoplot(outlier.end.plastics, series="Data") +
autolayer(trendcycle(decompose.outlier.end.plastics), series="trend") +
autolayer(seasadj(decompose.outlier.end.plastics), series="seasonally adjusted") +
xlab("Year") + ylab("Monthly Sales")
rm(list = ls())
library(fma)
library(fpp2)
books.original <- books
autoplot(books.original)
autoplot(books.original) +
xlab("Day") +
ylab("Book Sales") +
ggtitle("Daily Book Sales for Paperback and Hardcover Books")
books.ses = ses(books.original, h=5)
autoplot(books.ses) +
autolayer(fitted(session.ses), series="Fitted")
books.ses = ses(books.original, h=5)
books.ses = ses(books.original, h=5)
books.original
books.ses = ses(books.original(,'Paperback'), h=5)
books.ses = ses(books.original$Paperback, h=5)
books.ses = ses(books.original[,'Paperback'], h=5)
books.ses.paperback = ses(books.original[,'Paperback'], h=5)
books.ses.hardcover = ses(books.original[,'Hardcover'], h=5)
autoplot(books.ses) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted") +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
autoplot(books.original) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted") +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
autoplot(books.ses.paperback) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted")
autoplot(books.ses.hardcover) +
autolayer(fitted(books.ses.hardcover), series="Hardcover Fitted Data")
round(accuracy(books.ses.paperback),2)
round(accuracy(books.ses.hardcover),2)
books.holt.paperback <- holt(books.original[,'Paperback'])
books.holt.hardcover <- holt(books.original[,'Hardcover'])
books.holt.paperback <- holt(books.original[,'Paperback'], h=4)
books.holt.hardcover <- holt(books.original[,'Hardcover'], h=4)
autoplot(books.original[,'Paperback']) +
autolayer(books.holt.paperback, series="Holt's Method", PI=FALSE)
autoplot(books.original[,'Hardcover']) +
autolayer(books.holt.hardcover, series="Holt's Method", PI=FALSE)
round(accuracy(books.holt.paperback),2)
round(accuracy(books.holt.hardcover),2)
round(accuracy(books.holt.hardcover),2)
View(books.ses.hardcover)
View(books.ses.hardcover)
autoplot(books.original[,'Paperback']) +
autolayer(books.holt.paperback, series="Holt's Method", PI=FALSE)
autoplot(books.ses.paperback) +
autolayer(fitted(books.ses.paperback), series="Paperback Fitted")
books.ses.paperback$upper[1,'95%']
books.ses.paperback$lower[1,'95%']
books.ses.paperback.rsme <- round(accuracy(books.ses.paperback),2)
## Paperback RSME = 33.64
books.ses.hardcover.rsme <- round(accuracy(books.ses.hardcover),2)
## Hardcover RSME = 31.93
books.holt.paperback.rsme <- round(accuracy(books.holt.paperback),2)
## Paperback RSME = 31.14
books.holt.hardcover.rsme <- round(accuracy(books.holt.hardcover),2)
## Hardcover RSME = 27.19
books.ses.paperback$mean+1.96*
##2
## The plastics data set consists of the monthly sales (in thousands)
## of product A for a plastics manufacturer for five years.
## a. Plot the time series of sales of product A.
##    Can you identify seasonal fluctuations and/or a trend-cycle?
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
books.ses.paperback$upper[1,'95%']
books.ses.paperback$lower[1,'95%']
books.ses.paperback$mean+1.96* books.ses.paperback.rsme
books.ses.paperback$mean-1.96* books.ses.paperback.rsme
books.ses.paperback$mean + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
s <- sqrt(books.holt.paperback$model$mse)
s
View(books.holt.hardcover)
low <- books.holt.paperback$mean[1] - 1.96 * s
books.holt.hardcover
books.holt.paperback
c(low = low, high= high)
high <- books.holt.paperback$mean[1] + 1.96 * s
c(low = low, high= high)
s <- sqrt(books.holt.hardcover$model$mse)
high <- books.holt.hardcover$mean[1] + 1.96 * s
low <- books.holt.hardcover$mean[1] - 1.96 * s
books.holt.hardcover
c(low = low, high= high)
books.holt.paperback.level <- holt(books.original[,'Paperback'], h=4, level =95)
books.holt.paperback.level
books.holt.paperback
books.holt.hardcover
books.holt.paperback
rm(list = ls())
library(expsmooth)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
austa
library(fpp2)
austa
autoplot(austa)
autoplot(usgdp)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
austa %>% Arima(order=c(0,1,0), include.constant = FALSE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa.auto.arima <- austa %>% auto.arima()
austa.auto.arima %>% summary()
## ARIMA(0,1,1) with drift
## AIC = -15.24
## AICc = -14.46
austa.auto.arima %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(2,1,0), include.constant = TRUE)
austa %>% Arima(order=c(2,1,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(2,1,3), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(0,0,1), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(0,0,1), include.constant = TRUE)
austa %>% Arima(order=c(0,0,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
## AICc 108.03
## AIC 107.28
austa %>% Arima(order=c(0,0,0), include.constant = TRUE)
austa %>% Arima(order=c(0,2,1), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
rm(list = ls())
autoplot(usgdp)
autoplot(BoxCox(usgpd, BoxCox.lambda(usgdp)))
## De-trend series and reduce variances
autoplot(BoxCox(usgdp, BoxCox.lambda(usgdp)))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggPacf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% sumamry()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,0), include.constant=FALSE)
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,2))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% checkresiduals()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast() %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast(h=10) %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>%  auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
usgdp %>% auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
library(xlsx)
library(xlsx)
install.packages(c("backports", "curl", "digest", "ellipsis", "RcppArmadillo", "rlang", "TTR"))
install.packages(c("backports", "curl", "digest", "ellipsis", "RcppArmadillo", "rlang", "TTR"))
library("xlsx")
correlations.20.observations <- cor(df.20.observations)
correlations.20.observations <- cor(df.20.observations)
getwd()
wd <- "C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B"
setwd(wd);
read.table("Random data v2")
## Import Data
data <- read.csv("Random data v2.csv", header=FALSE)
typeof(data)
## Exercise:
## The file Some Random Numbers to Play With contains 20,000 random numbers generated by a high quality
## method from Random.org. They have been arranged, maintaining full randomness, into 1,000 rows with 20
## columns in each row.
##
## These data can be viewed in two fundamentally different ways. One approach is to consider them as twenty
## columns or features with 1000 data points for each. The other is to view them as 1,000 rows or features
## with 20 data points each.
## Explore the behavior of correlations using each of the two views. Describe any patterns or surprises you
## observe in these correlations, especially given that these data are highly random. How do your observations
## fir with Granville's first article?
## Optional: Explore some of the data sources in the Spurious Correlations piece, or other data sources if
## you wish, using some of the alternative approaches to correlation. If you use Granville's t, you'll need to
## do some coding. Can you identify circumstances in which the non-parametric approach might better inform a
## model than the traditional Pearson's R?
## Get Data in Both 1000 observation format & 20 observation format
df.1000.observations <- data.frame(data)
df.20.observations <- data.frame(t(df.1000.observations))
df.20.observations <- sapply( df.20.observations, as.numeric )
df.1000.observations <- sapply( df.1000.observations, as.numeric )
correlations.1000.observations <- cor(df.1000.observations)
##correlations.20.observations <- cor(df.20.observations)
View(correlations.1000.observations)
View(correlations.1000.observations)
correlations.20.observations <- cor(df.20.observations)
View(df.20.observations)
View(df.20.observations)
View(df.20.observations)
View(df.20.observations)
View(correlations.20.observations)
View(correlations.20.observations)
View(df.1000.observations)
View(df.1000.observations)
View(df.20.observations)
View(df.20.observations)
df.20.observations <- data.frame(t(df.1000.observations))
df.20.observations
df.20.observations <- data.frame(df.1000.observations)
df.20.observations
df.20.observations <- data.frame(t(df.1000.observations))
df.20.observations
df.20.observations <- data.frame(t(data))
df.20.observations <- sapply( df.20.observations, as.numeric )
df.1000.observations <- data.frame(data)
df.1000.observations <- sapply( df.1000.observations, as.numeric )
df.20.observations <- t(df.1000.observations)
correlations.1000.observations <- cor(df.1000.observations)
correlations.20.observations <- cor(df.20.observations)
View(df.20.observations)
View(df.20.observations)
View(df.1000.observations)
View(df.1000.observations)
library("ggpubr")
install.packages("ggpubr")
library("ggpubr")
View(df.1000.observations)
View(df.1000.observations)
View(df.20.observations)
View(df.20.observations)
cor(df.1000.observations["v1"],df.1000.observations["v2"], method=c("pearson","kendall","spearman"))
cor(df.1000.observations["v1"],df.1000.observations["v2"], method=c("pearson","kendall","spearman"))
cor(df.1000.observations$v1,df.1000.observations$v2, method=c("pearson","kendall","spearman"))
cor(df.1000.observations[,"v1"],df.1000.observations[,"v2"], method=c("pearson","kendall","spearman"))
df.1000.observations[,"v1"]
df.1000.observations[,"V1"]
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson","kendall","spearman"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.100.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.100.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("spearman"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("pearson"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("kendall"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("pearson"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("kendall"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
View(df.20.observations)
View(df.20.observations)
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("pearson"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
cor(df.20.observations[,"V1"] , df.20.observations[,"V2"] ,  method=c("pearson"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("kendall"))
cor(df.20.observations[,"V1"],df.20.observations[,"V2"], method=c("spearman"))
correlations.20.observations <- cor(df.20.observations)
df.20.observations <- t(df.1000.observations)
cor(df.20.observations[,"V1"]  ,df.20.observations[,"V2"] ,  method=c("pearson"))
cor(df.20.observations[,"V1"]  ,df.20.observations[,"V2"],   method=c("kendall"))
cor(df.20.observations[,"V1"]  ,df.20.observations[,"V2"],   method=c("spearman"))
df.20.observations
cor(df.20.observations[,1]  ,df.20.observations[,2] ,  method=c("pearson"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("kendall"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("spearman"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("spearman"))
cor(df.20.observations[,1]  ,df.20.observations[,2] ,  method=c("pearson"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("kendall"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("spearman"))
df.20.observations <- t(df.1000.observations)
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("spearman"))
cor(df.20.observations[,1]  ,df.20.observations[,2] ,  method=c("pearson"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("kendall"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("spearman"))
cor(df.20.observations[,1]  ,df.20.observations[,2] ,  method=c("pearson"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("kendall"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("spearman"))
cor(df.1000.observations[,"V3"],df.1000.observations[,"V4"], method=c("pearson"))
cor(df.1000.observations[,"V3"],df.1000.observations[,"V4"], method=c("kendall"))
cor(df.1000.observations[,"V3"],df.1000.observations[,"V4"], method=c("spearman"))
cor(df.20.observations[,3]  ,df.20.observations[,4] ,  method=c("pearson"))
cor(df.20.observations[,3]  ,df.20.observations[,4],   method=c("kendall"))
cor(df.20.observations[,3]  ,df.20.observations[,4],   method=c("spearman"))
cor(df.20.observations  ,df.20.observations ,  method=c("pearson"))
rm(list = ls())
library(lmtest)
head(ChickEgg)
plot.ts(ChickEgg)
grangertest(chicken ~ egg, order =3, data=ChickEgg)
grangertest(egg ~ chicken, order =3, data=ChickEgg)
