getwd()

wd <- "C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B"
setwd(wd);

read.table("Random data v2")

## Import Data
data <- read.csv("Random data v2.csv", header=FALSE)

typeof(data)


## Exercise:
## The file Some Random Numbers to Play With contains 20,000 random numbers generated by a high quality 
## method from Random.org. They have been arranged, maintaining full randomness, into 1,000 rows with 20
## columns in each row.
##
## These data can be viewed in two fundamentally different ways. One approach is to consider them as twenty
## columns or features with 1000 data points for each. The other is to view them as 1,000 rows or features 
## with 20 data points each.

## Explore the behavior of correlations using each of the two views. Describe any patterns or surprises you 
## observe in these correlations, especially given that these data are highly random. How do your observations 
## fir with Granville's first article?

## Optional: Explore some of the data sources in the Spurious Correlations piece, or other data sources if
## you wish, using some of the alternative approaches to correlation. If you use Granville's t, you'll need to
## do some coding. Can you identify circumstances in which the non-parametric approach might better inform a 
## model than the traditional Pearson's R?

## Get Data in Both 1000 observation format & 20 observation format
df.1000.observations <- data.frame(data)
df.1000.observations <- sapply( df.1000.observations, as.numeric )

df.20.observations <- t(df.1000.observations)



library("ggpubr")

cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("pearson"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("kendall"))
cor(df.1000.observations[,"V1"],df.1000.observations[,"V2"], method=c("spearman"))

## 1000 OBSERATIONS:
## PEARSON:  -.0759821
## KENDALL:  -.05214414
## SPEARMAN: -.07676264

cor(df.20.observations[,1]  ,df.20.observations[,2] ,  method=c("pearson"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("kendall"))
cor(df.20.observations[,1]  ,df.20.observations[,2],   method=c("spearman"))

## PEARSON:   .9999928
## KENDALL:  -.05263158
## SPEARMAN: -.08571429

## the wider the data set and less observations, Pearson's R is more likely
## to incorrectly calculate the correlation.


########################################################## ############
## NOTES OVER CONTENT
## Pearson's R (Refresher)
  ## Measures the linear strength of the relationship between two variables

  ## p when applies to population, r in a sample
  
  ## Between -1 to 1, -1 indicates negative relatiionship, 0 no relationship
  ## 1 positive relationshp.

  ## symmetric, relationship on X w/ Y is the same Y w/ X.

  ## Unaffected by linear transformations

## Granville argues:
  ## When looking for patterns in a very large data sets, coincidences
  ## in the data will appear as strong patterns: caused by chance, not replicable
  ## no predictive power
  ## hides weaker patterns that are better features that can be used for predictions

  ## perform cross-correlation on all metrics (exploratory analysis)
  ## spectral analysis of normalized data instead of correlation to identify relationships

  ## k # of time series datasets
  ## n obersvations
  ## curse of big data occurs when n < 200, k >= 500.
  ## When n is larger, the curse of big data isn't a problem, but instances were
  ## n and k are large (n > 1000 and k > 5000) is rare

  ## m = # of bi-variate independent data from k
  ## assuming independent ts, each having the same # of n (random #),
  ## what are the chances one of m correlation coefficients > .8?
  ## In theory = 0
  ## Actuality if n=20, m=10,000 the probability that r > .8 = 90.93%
  ## if n = 20, n=100,000 = 98.17%

  ## Pratical cross-correlations are performed on dependent paired datasets
  ## (violating independence assumptions), not Gaussian (theoretically r isn't 0),
  ## and m can be much larger.

  rm(list = ls())
 