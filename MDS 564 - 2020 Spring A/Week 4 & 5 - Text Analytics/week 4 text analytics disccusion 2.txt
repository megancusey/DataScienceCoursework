The most basic concept discussed in Feature Engineering for Machine Learning this week was the concept of "Bag of Words". This is just the frequency of seeing a distinct word in a document. As a result, the new vector of numbers containing the word count number represents the document converting it from a textual structure to a numeric one. While converting a text document into numeric is important, there are several disadvantages to the BOW approach. Each unique word is represented by a dimension. If a document, or collection of documents have a large number of unique words, then the number of dimensions will grow quickly. Also, the context of the text may get lost when "chopping" the words up. For example "not sorry" has the opposite meaning of "sorry".

To combat the loss of meaning BOW creates, you may choose to use n-grams instead. n-grams subsets the words by n words at a time. For example, if you create a 2-gram then each sequence of two words are represented in a dimension. As n gets larger, the number of dimensions exponentially increase. This once again leads to feature bloat and expensive computational time.

Stopword lists can be used to filter out common words that don't provide much value to the algorithm. Another way to filter out words (aka dimensions) is to identify very common and very rare terms. Words that occur very frequently or very rarely are not good predictors. Rare words often dominate a dataset so removing rare words increases efficiency. Stemming is also a way to reduce dimensions. Stemming focuses on trying to identify the root word. For example if a sentence contains "She runs" and another "He is running", then stemming seeks to reduce what would be 2 dimensions created by BOW (runs and running) to 1, "run", because run is the root word of each of those words. However, sometimes stemming creates additional problems as there is always words that do not follow convential subfixes.

Other topics of text preprocessing include collaction which performs a hypothesis test to identify meaningful combination of words, not necessarily right next to each other in a sentence, chunking and tagging parts of speech, and term frequency-inverse document frequency. Tf-idf essentially penalizes a word for the number of documents the word appears in. The most common words will all hover around a value of 1 while least frequent words will have a higher value.

