What You Need to Know about Classical Statistics

This text gave nice break down of the difference between classical statistics and Bayesian Theory, in what instances are they useful, and what are some of the pros and cons.

I found it entertaining that when talking about classical statistics it was explained as the focus of all Introductory Statistics courses when the methods that these ideas deririved from were to overcome difficulties we no longer have today such as limited computing power, graphing capabilities, and data collection. While classical statistics can still be useful, there's certainly other methods that can turn what was a disadvantage into an asset and produce more interesting and useful results.

Some important key concepts I took away from this text is that classical statistics is based on the assumption that the data is normally distributed. Bayesian Statistics does not make this same assumption, so as a result, applying the Bayesian Theory can be more difficult to determine all the parameters. Another interesting insight I got from this text is that hypothesis testing for classical statistics is only really good for measuring treatments. Otherwise, it's not really applicable because it only tells you how big an effect of something on an experiment is. Hypothesis are accepted or rejected based on an arbitruary significance level (who comes up with the significane level) and this determines their result instead of also observing how close to the significance level the experiment got? With a bigger sample size, the results of an experiment could differ. In contrast, Bayes Theorum is also a bit limited because you HAVE to have a hypothesis or question to answer to get started in order to begin filling in the parameters. It's applicable to exploratory research.