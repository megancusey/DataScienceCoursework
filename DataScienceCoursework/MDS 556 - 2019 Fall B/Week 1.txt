Module 1 readings hovered around the ideas that too many features lead to overfitting and that selecting the most valuable features can be a make it or break it factor when applying machine learning techniques.

Discover Feature Engineering, How to Engineer Features, and How to Get Good At It, discussed the importances of feature engineering. In particular, it made a great point that if you have features that fit the problem in a way the algorithm understands, you have more flexibility in your parameters and which models you can choose to apply all while producing good results. Sometimes feature engineering can mean finding the best model and evaluation metrics for your model. Other times it can be finding the most highly correlated feature(s) to the target attribute or creating manual features that make information more available to the model.

The Curse of Dimensionality article circled back to some concepts discussed earlier in this program. There is a "sweet" spot when it comes to applying features to a model. When too many or too few dimensions have been applied to a model, the performance of the model exponentially suffers. If there is many training data available, more dimensions may be able to be applied without overfitting. In contrast, less training data means that you might be able to only get by with a few. Feature selection, feature extraction, and cross validation techniques can all aid in limiting dimensionality to only what is neccessary to obtain the "sweet" spot and optimize model results.