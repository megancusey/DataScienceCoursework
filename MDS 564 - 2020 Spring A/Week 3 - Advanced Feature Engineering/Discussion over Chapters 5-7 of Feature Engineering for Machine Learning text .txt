There was a lot of interesting concepts taught in this weeks assigned chapters of our text. The following lists a summary of these concepts and my take aways.

Chapter 5 discussed ways to numerically represent categorical variables. Three potential solutions to "encoding" categorical variables is to create boolean/binary values to represent the presence of a category. One-Hot Encoding, Dummy Coding, and Effect Coding are the more popular ways to accomplish this task and each has their pros/cons. One major con to this process is that it is much more suitable for cateogries with fewer levels for each level adds a dimension. This when more advanced tools such as Feature Hashing and Bin Counting are more appropriate vs the aforementioned methods.

Chapter 6 covered our old friend PCA. While I admit to not having the best comprehension of linear algebra, I certainly appreciated the book's take on describing PCA. In particular, it made since that PCA centers the data by removing the mean from the data points to make the overall equation more simple so that the variance that is being maximized is an expression of the expectation of Z^2. I also made a special note that PCA is useful for removing correlations between input variables so it could be useful for a dataset that is difficult to remove multicollinearity. 

Chapter 7 discussed using k-means clustering to assign groups to data points and then use k-nn as a input variable to a linear regression model. I found this to be a really excellent concept with a lot of power behind it. I hope to put it practice as soon as I get the chance.

One idea that I am trying to solidify is how to help limit data leakage when using bin counting and model statcking (when the target variable is used). I know it's been said before to use a different part of your data set to perform the statistics, but what does this look like? For example, do people use 80% for the training set and 20% for the test set AND THEN 20% for bin counting/model stacking, 20% for cross validation, 40% for actual model training? What if there just simply aren't enough observations?